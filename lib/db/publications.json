[
  {
    "type": "article-journal",
    "title": "Asymptotic-Preserving Neural Networks based on Even-odd Decomposition for Multiscale Gray Radiative Transfer Equations",
    "author": [
      {
        "family": "Wu",
        "given": "Keke"
      },
      {
        "family": "Xie",
        "given": "Xizhe"
      },
      {
        "family": "Chen",
        "given": "Wengu"
      },
      {
        "family": "Wang",
        "given": "Han"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2501.08166",
    "arXiv": "2501.08166",
    "abstract": "We present a novel Asymptotic-Preserving Neural Network (APNN) approach utilizing even-odd decomposition to tackle the nonlinear gray radiative transfer equations (GRTEs). Our AP loss demonstrates consistent stability concerning the small Knudsen number, ensuring the neural network solution uniformly converges to the macro solution. This APNN method alleviates the rigorous conservation requirements while simultaneously incorporating an auxiliary deep neural network, distinguishing it from the APNN method based on micro-macro decomposition for GRTE. Several numerical problems are examined to demonstrate the effectiveness of our proposed APNN technique.",
    "issued": {
      "date-parts": [["2025"]]
    },
    "journalAbbreviation": "arXiv",
    "id": "c84fe6f7-e56b-4aa7-8eb8-d3b265f8662b",
    "tags": ["APNNs", "Gray-RTE"]
  },
  {
    "type": "article-journal",
    "title": "DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer",
    "author": [
      {
        "family": "Zhu",
        "given": "Yekun"
      },
      {
        "family": "Tang",
        "given": "Min"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2505.23190",
    "arXiv": "2505.23190",
    "abstract": "In this paper, we propose a novel neural network approach, termed DeepRTE, to address the steady-state Radiative Transfer Equation (RTE). The RTE is a differential-integral equation that governs the propagation of radiation through a participating medium, with applications spanning diverse domains such as neutron transport, atmospheric radiative transfer, heat transfer, and optical imaging. Our DeepRTE framework demonstrates superior computational efficiency for solving the steady-state RTE, surpassing traditional methods and existing neural network approaches. This efficiency is achieved by embedding physical information through derivation of the RTE and mathematically-informed network architecture. Concurrently, DeepRTE achieves high accuracy with significantly fewer parameters, largely due to its incorporation of mechanisms such as multi-head attention. Furthermore, DeepRTE is a mesh-free neural operator framework with inherent zero-shot capability. This is achieved by incorporating Green's function theory and pre-training with delta-function inflow boundary conditions into both its architecture design and training data construction. The efficacy of the proposed approach is substantiated through comprehensive numerical experiments.",
    "issued": {
      "date-parts": [["2025"]]
    },
    "journalAbbreviation": "arXiv",
    "id": "9e940c07-a813-4198-910e-eebc28675dba",
    "tags": ["DeepRTE", "Operator Leraning"]
  },
  {
    "type": "article-journal",
    "title": "Capturing Shock Waves by Relaxation Neural Networks",
    "author": [
      {
        "family": "Zhou",
        "given": "Nan"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2404.01163",
    "arXiv": "2404.01163",
    "abstract": "In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems. This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN). It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems' solutions. This ultimately results in the failure of optimization that is based on gradient descent in the training process. Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective. Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework. In addition to the remarkable results demonstrated in numerical simulations, most of the acceleration techniques and improvement strategies aimed at the standard PINN framework can also be applied to the RelaxNN framework.",
    "issued": {
      "date-parts": [["2024"]]
    },
    "journalAbbreviation": "arXiv",
    "id": "b1dbf591-6c07-4016-9539-ccad0d3845f3",
    "tags": ["Shocks", "Conservation Laws", "PINNs"]
  },
  {
    "type": "article-journal",
    "title": "RT-APNN for Solving Gray Radiative Transfer Equations",
    "author": [
      {
        "family": "Xie",
        "given": "Xizhe"
      },
      {
        "family": "Chen",
        "given": "Wengu"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Wang",
        "given": "Han"
      }
    ],
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2505.14144",
    "arXiv": "2505.14144",
    "abstract": "The Gray Radiative Transfer Equations (GRTEs) are high-dimensional, multiscale problems that pose significant computational challenges for traditional numerical methods. Current deep learning approaches, including Physics-Informed Neural Networks (PINNs) and Asymptotically Preserving Neural Networks (APNNs), are largely restricted to low-dimensional or linear GRTEs. To address these challenges, we propose the Radiative Transfer Asymptotically Preserving Neural Network (RT-APNN), an innovative framework extending APNNs. RT-APNN integrates multiple neural networks into a cohesive architecture, reducing training time while ensuring high solution accuracy. Advanced techniques such as pre-training and Markov Chain Monte Carlo (MCMC) adaptive sampling are employed to tackle the complexities of long-term simulations and intricate boundary conditions. RT-APNN is the first deep learning method to successfully simulate the Marshak wave problem. Numerical experiments demonstrate its superiority over existing methods, including APNNs and MD-APNNs, in both accuracy and computational efficiency. Furthermore, RT-APNN excels at solving high-dimensional, nonlinear problems, underscoring its potential for diverse applications in science and engineering.",
    "issued": {
      "date-parts": [["2025"]]
    },
    "journalAbbreviation": "arXiv",
    "id": "576e039b-72a9-45e6-afe6-46f4d94f7492",
    "tags": ["APNNs", "RT-APNN", "Gray-RTE"]
  },
  {
    "type": "article-journal",
    "title": "An Unsupervised Deep Learning Approach for the Wave Equation Inverse Problem",
    "author": [
      {
        "family": "Yan",
        "given": "Xiong-Bin"
      },
      {
        "family": "Wu",
        "given": "Keke"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2311.04531",
    "arXiv": "2311.04531",
    "abstract": "Full-waveform inversion (FWI) is a powerful geophysical imaging technique that infers high-resolution subsurface physical parameters by solving a non-convex optimization problem. However, due to limitations in observation, e.g., limited shots or receivers, and random noise, conventional inversion methods are confronted with numerous challenges, such as the local-minimum problem. In recent years, a substantial body of work has demonstrated that the integration of deep neural networks and partial differential equations for solving full-waveform inversion problems has shown promising performance. In this work, drawing inspiration from the expressive capacity of neural networks, we provide an unsupervised learning approach aimed at accurately reconstructing subsurface physical velocity parameters. This method is founded on a re-parametrization technique for Bayesian inference, achieved through a deep neural network with random weights. Notably, our proposed approach does not hinge upon the requirement of the labeled training dataset, rendering it exceedingly versatile and adaptable to diverse subsurface models. Extensive experiments show that the proposed approach performs noticeably better than existing conventional inversion methods.",
    "issued": {
      "date-parts": [["2023"]]
    },
    "journalAbbreviation": "arXiv",
    "id": "e25f1255-eca2-467d-bff5-77ceeeb5b51c",
    "tags": ["", "FWI", "Deep Learning", "Unsupervised Learning"]
  },
  {
    "type": "article-journal",
    "title": "ODE-DPS: ODE-Based Diffusion Posterior Sampling for Linear Inverse Problems in Partial Differential Equation",
    "author": [
      {
        "family": "Jiang",
        "given": "Enze"
      },
      {
        "family": "Peng",
        "given": "Jishen"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Yan",
        "given": "Xiong-Bin"
      }
    ],
    "container-title": "Journal of Scientific Computing",
    "ISSN": "0885-7474",
    "DOI": "10.1007/s10915-025-02790-8",
    "abstract": "In recent years we have witnessed a growth in mathematics for deep learning, which has been used to solve inverse problems of partial differential equations (PDEs). However, most deep learning-based inversion methods either require paired data or necessitate retraining neural networks for modifications in the conditions of the inverse problem, significantly reducing the efficiency of inversion and limiting its applicability. To overcome this challenge, in this paper, leveraging the score-based generative diffusion model, we introduce a novel unsupervised inversion methodology tailored for solving inverse problems arising from PDEs. Our approach operates within the Bayesian inversion framework, treating the task of solving the posterior distribution as a conditional generation process achieved through solving a reverse-time stochastic differential equation. Furthermore, to enhance the accuracy of inversion results, we propose an ODE-based Diffusion Posterior Sampling inversion algorithm. The algorithm stems from the marginal probability density functions of two distinct forward generation processes that satisfy the same Fokker–Planck equation. Through a series of experiments involving various PDEs, we showcase the efficiency and robustness of our proposed method.",
    "issued": {
      "date-parts": [["2025"]]
    },
    "journalAbbreviation": "J. Sci. Comput.",
    "page": "69",
    "issue": "3",
    "volume": "102",
    "id": "e7b0ddfc-5bf4-4d91-9d64-82e2aaa81e43",
    "tags": ["Inverse problems", "ODE-DPS", "Diffusion models"]
  },
  {
    "type": "article-journal",
    "title": "A micro-macro decomposition-based asymptotic-preserving random feature method for multiscale radiative transfer equations",
    "author": [
      {
        "family": "Chen",
        "given": "Jingrun"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Wu",
        "given": "Keke"
      }
    ],
    "container-title": "Journal of Computational Physics",
    "ISSN": "0021-9991",
    "DOI": "10.1016/j.jcp.2025.114103",
    "abstract": "This paper introduces the Asymptotic-Preserving Random Feature Method (APRFM) for the efficient resolution of multiscale radiative transfer equations. The APRFM effectively addresses the challenges posed by stiffness and multiscale characteristics inherent in radiative transfer equations through the application of a micro-macro decomposition strategy. This approach decomposes the distribution function into equilibrium and non-equilibrium components, allowing for the approximation of both parts through the random feature method (RFM) within a least squares minimization framework. The proposed method exhibits remarkable robustness across different scales and achieves high accuracy with fewer degrees of freedom and collocation points than the vanilla RFM. Additionally, compared to the deep neural network-based method, our approach offers significant advantages in terms of parameter efficiency and computational speed. These benefits have been substantiated through numerous numerical experiments conducted on both one- and two-dimensional problems.",
    "issued": {
      "date-parts": [["2025"]]
    },
    "journalAbbreviation": "J. Comput. Phys.",
    "page": "114103",
    "volume": "537",
    "id": "e05fc805-656f-49bf-9999-1e655aef5bd7",
    "tags": ["Radiative transfer", "APNNs", "Random feature method"]
  },
  {
    "type": "article-journal",
    "title": "On the Exact Computation of Linear Frequency Principle Dynamics and Its Generalization",
    "author": [
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      }
    ],
    "container-title": "SIAM Journal on Mathematics of Data Science",
    "DOI": "10.1137/21m1444400",
    "issued": {
      "date-parts": [["2022"]]
    },
    "journalAbbreviation": "SIAM J. Math. Data Sci.",
    "page": "1272-1292",
    "issue": "4",
    "volume": "4",
    "id": "3df96e41-f2c3-4689-ae3f-7c697d02e456",
    "tags": ["Deep Leraning Theory", "F-principle"]
  },
  {
    "type": "article-journal",
    "title": "Heat flux estimation of the cylinder in hypersonic rarefied flow based on neural network surrogate model",
    "author": [
      {
        "family": "Ding",
        "given": "Dongming"
      },
      {
        "family": "Chen",
        "given": "Hao"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Zhang",
        "given": "Bin"
      },
      {
        "family": "Liu",
        "given": "Hong"
      }
    ],
    "container-title": "AIP Advances",
    "DOI": "10.1063/5.0108757",
    "abstract": "An efficient method to predict thermal loads on hypersonic vehicles in rarefied flows is immediately needed, especially when designing the thermal protection system. To meet the demand, we combine artificial neural networks with the direct simulation Monte Carlo method and build the surrogate model for hypersonic rarefied flows with three inputs (Knudsen number, temperature ration, and Mach number). The heating coefficients at nine points along the surface of a two-dimensional cylinder are output from the model. The results at the stagnation point have errors within 3%, while the biggest error of nine points is 4.8%. The heating coefficients are also compared with the bridge function’s, whose errors reach 14% at the stagnation point and 20% along the surface. The reasons for the errors are discussed in detail. In addition, this framework of building the model with artificial neural networks can be extended to solve problems with more complex mechanisms or configurations.",
    "issued": {
      "date-parts": [["2022"]]
    },
    "journalAbbreviation": "AIP Adv.",
    "page": "085314",
    "issue": "8",
    "volume": "12",
    "id": "a5db1303-c6d0-47e6-b7c2-19cc4f00350b",
    "tags": ["Deep Learning", "Surrogate Modeling", "Rarefied Flow"]
  },
  {
    "type": "article-journal",
    "title": "Laplace-fPINNs: Laplace-Based Fractional Physics-Informed Neural Networks for Solving Forward and Inverse Problems of a Time Fractional Equation",
    "author": [
      {
        "family": "Yan",
        "given": "Xiong-Bin"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "East Asian Journal on Applied Mathematics",
    "ISSN": "2079-7362",
    "DOI": "10.4208/eajam.2023-197.171223",
    "issued": {
      "date-parts": [["2024"]]
    },
    "journalAbbreviation": "East Asian J. Appl. Math.",
    "page": "657-674",
    "issue": "4",
    "volume": "14",
    "id": "1c621cf2-cc78-4b2b-bc13-1e8c4868a35a",
    "tags": ["PINNs", "Laplace-fPINNs", "Fractional"]
  },
  {
    "type": "article-journal",
    "title": "Bayesian Inversion with Neural Operator (BINO) for modeling subdiffusion: Forward and inverse problems",
    "author": [
      {
        "family": "Yan",
        "given": "Xiong-Bin"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "Journal of Computational and Applied Mathematics",
    "ISSN": "0377-0427",
    "DOI": "10.1016/j.cam.2024.116191",
    "abstract": "Fractional diffusion equations have been an effective tool for modeling anomalous diffusion in complicated systems. However, traditional numerical methods require expensive computation cost and storage resources because of the memory effect brought by the convolution integral of time fractional derivative. We propose a Bayesian Inversion with Neural Operator (BINO) to overcome the difficulty in traditional methods as follows. We employ a deep operator network to learn the solution operators for the fractional diffusion equations, allowing us to swiftly and precisely solve a forward problem for given inputs (including fractional order, diffusion coefficient, source terms, etc.). In addition, we integrate the deep operator network with a Bayesian inversion method for modeling a problem by subdiffusion process and solving inverse subdiffusion problems, which reduces the time costs (without suffering from overwhelm storage resources) significantly. A large number of numerical experiments demonstrate that the operator learning method proposed in this work can efficiently solve the forward problems and Bayesian inverse problems of the subdiffusion equation.",
    "issued": {
      "date-parts": [["2025"]]
    },
    "journalAbbreviation": "J. Comput. Appl. Math.",
    "page": "116191",
    "volume": "454",
    "id": "66b0f0a0-ce7a-4552-9caa-fb05c80bb53a",
    "tags": [
      "Inverse problems",
      "Fractional Diffusion",
      "Operator Leraning",
      "Bayesian"
    ]
  },
  {
    "type": "article-journal",
    "title": "Asymptotic-Preserving Neural Networks for Multiscale Vlasov–Poisson–Fokker–Planck System in the High-Field Regime",
    "author": [
      {
        "family": "Jin",
        "given": "Shi"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Zhang",
        "given": "Tian-ai"
      }
    ],
    "container-title": "Journal of Scientific Computing",
    "ISSN": "0885-7474",
    "DOI": "10.1007/s10915-024-02527-z",
    "abstract": "The Vlasov–Poisson–Fokker–Planck (VPFP) system is a fundamental model in plasma physics that describes the Brownian motion of a large ensemble of particles within a surrounding bath. Under the high-field scaling, both collision and field are dominant. This paper introduces two Asymptotic-Preserving Neural Network (APNN) methods within a physics-informed neural network (PINN) framework for solving the VPFP system in the high-field regime. These methods aim to overcome the computational challenges posed by high dimensionality and multiple scales of the system. The first APNN method leverages the micro–macro decomposition model of the original VPFP system, while the second is based on the mass conservation law. Both methods ensure that the loss function of the neural networks transitions naturally from the kinetic model to the high-field limit model, thereby preserving the correct asymptotic behavior. Through extensive numerical experiments, these APNN methods demonstrate their effectiveness in solving multiscale and high dimensional uncertain problems, as well as their broader applicability for problems with long time duration and non-equilibrium initial data.",
    "issued": {
      "date-parts": [["2024"]]
    },
    "journalAbbreviation": "J. Sci. Comput.",
    "page": "61",
    "issue": "3",
    "volume": "99",
    "id": "7ccff08f-3a88-4b30-aab2-84a6fd23861b",
    "tags": [
      "APNNs",
      "Multiscale Kinetic Equations",
      "High-Field Scaling",
      "Vlasov–Poisson–Fokker–Planck "
    ]
  },
  {
    "type": "article-journal",
    "title": "Asymptotic-Preserving Neural Networks for Multiscale Kinetic Equations",
    "author": [
      {
        "family": "Jin",
        "given": "Shi"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Wu",
        "given": "Keke"
      }
    ],
    "container-title": "Communications in Computational Physics",
    "ISSN": "1815-2406",
    "DOI": "10.4208/cicp.oa-2023-0211",
    "issued": {
      "date-parts": [["2024"]]
    },
    "journalAbbreviation": "Commun. Comput. Phys.",
    "page": "693-723",
    "issue": "3",
    "volume": "35",
    "id": "53a8e452-4709-499f-9efd-44f357d68258",
    "tags": ["APNNs", "Multiscale Kinetic Equations"]
  },
  {
    "type": "article-journal",
    "title": "Capturing the diffusive behavior of the multiscale linear transport equations by Asymptotic-Preserving Convolutional DeepONets",
    "author": [
      {
        "family": "Wu",
        "given": "Keke"
      },
      {
        "family": "Yan",
        "given": "Xiong-Bin"
      },
      {
        "family": "Jin",
        "given": "Shi"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "Computer Methods in Applied Mechanics and Engineering",
    "ISSN": "0045-7825",
    "DOI": "10.1016/j.cma.2023.116531",
    "abstract": "In this paper, we introduce two types of novel Asymptotic-Preserving Convolutional Deep Operator Networks (APCONs) designed to solve the multiscale time-dependent linear transport equations. We observe that the vanilla physics-informed DeepONets with modified MLP may exhibit instability in maintaining the desired limiting macroscopic behavior. Therefore, this necessitates the utilization of an asymptotic-preserving loss function. Drawing inspiration from the heat kernel in the diffusion equation, we propose a new architecture called Convolutional Deep Operator Networks, which employs multiple local convolution operations instead of a global heat kernel, along with pooling and activation operations in each filter layer. Our APCON methods possess a parameter count that is independent of the grid size and are capable of capturing the diffusive behavior of the linear transport problem. Finally, we validate the effectiveness of our methods through several numerical examples.",
    "issued": {
      "date-parts": [["2024"]]
    },
    "journalAbbreviation": "Comput. Methods Appl. Mech. Eng.",
    "page": "116531",
    "volume": "418",
    "id": "7db6a4f5-a551-4709-b724-06e31a41930a",
    "tags": [
      "APNNs",
      "APCON",
      "Multiscale Kinetic Equations",
      "Operator Leraning"
    ]
  },
  {
    "type": "article-journal",
    "title": "Asymptotic-Preserving Neural Networks for Multiscale Time-Dependent Linear Transport Equations",
    "author": [
      {
        "family": "Jin",
        "given": "Shi"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Wu",
        "given": "Keke"
      }
    ],
    "container-title": "Journal of Scientific Computing",
    "ISSN": "0885-7474",
    "DOI": "10.1007/s10915-023-02100-0",
    "abstract": "In this paper we develop a neural network for the numerical simulation of time-dependent linear transport equations with diffusive scaling and uncertainties. The goal of the network is to resolve the computational challenges of curse-of-dimensionality and multiple scales of the problem. We first show that a standard Physics-Informed Neural Network (PINN) fails to capture the multiscale nature of the problem, hence justifies the need to use Asymptotic-Preserving Neural Networks (APNNs). We show that not all classical AP formulations are directly fit for the neural network approach. We construct a micro-macro decomposition based neural network, and also build in a mass conservation mechanism into the loss function, in order to capture the dynamic and multiscale nature of the solutions. Numerical examples are used to demonstrate the effectiveness of this APNNs.",
    "issued": {
      "date-parts": [["2023"]]
    },
    "journalAbbreviation": "J. Sci. Comput.",
    "page": "57",
    "issue": "3",
    "volume": "94",
    "id": "0f2ccbef-7c70-4a5c-8e50-9fa302608033",
    "tags": [
      "APNNs",
      "Linear Transport Equations",
      "Uncertainty Quantification"
    ]
  },
  {
    "type": "article-journal",
    "title": "MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs",
    "author": [
      {
        "family": "Zhang",
        "given": "Lulu"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "E",
        "given": "Weinan"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "Communications in Computational Physics",
    "ISSN": "1815-2406",
    "DOI": "10.4208/cicp.oa-2021-0257",
    "arXiv": "2107.03673",
    "abstract": "In this paper, we propose a a machine learning approach via model-operator-data network (MOD-Net) for solving PDEs. A MOD-Net is driven by a model to solve PDEs based on operator representation with regularization from data. For linear PDEs, we use a DNN to parameterize the Green's function and obtain the neural operator to approximate the solution according to the Green's method. To train the DNN, the empirical risk consists of the mean squared loss with the least square formulation or the variational formulation of the governing equation and boundary conditions. For complicated problems, the empirical risk also includes a few labels, which are computed on coarse grid points with cheap computation cost and significantly improves the model accuracy. Intuitively, the labeled dataset works as a regularization in addition to the model constraints. The MOD-Net solves a family of PDEs rather than a specific one and is much more efficient than original neural operator because few expensive labels are required. We numerically show MOD-Net is very efficient in solving Poisson equation and one-dimensional radiative transfer equation. For nonlinear PDEs, the nonlinear MOD-Net can be similarly used as an ansatz for solving nonlinear PDEs, exemplified by solving several nonlinear PDE problems, such as the Burgers equation.",
    "issued": {
      "date-parts": [["2022"]]
    },
    "journalAbbreviation": "Commun. Comput. Phys.",
    "page": "299-335",
    "issue": "2",
    "volume": "32",
    "id": "4dbdddd2-e14e-4e4d-894f-de86b5f8e972",
    "tags": ["Operator Leraning", "MOD-Net"]
  },
  {
    "type": "article-journal",
    "title": "Uniformly accurate machine learning-based hydrodynamic models for kinetic equations",
    "author": [
      {
        "family": "Han",
        "given": "Jiequn"
      },
      {
        "family": "Ma",
        "given": "Chao"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "E",
        "given": "Weinan"
      }
    ],
    "container-title": "Proceedings of the National Academy of Sciences",
    "ISSN": "0027-8424",
    "DOI": "10.1073/pnas.1909854116",
    "PMID": "31619568",
    "PMCID": "PMC6825311",
    "arXiv": "1907.03937",
    "abstract": "A framework is introduced for constructing interpretable and truly reliable reduced models for multiscale problems in situations without scale separation. Hydrodynamic approximation to the kinetic equation is used as an example to illustrate the main steps and issues involved. To this end, a set of generalized moments are constructed first to optimally represent the underlying velocity distribution. The well-known closure problem is then solved with the aim of best capturing the associated dynamics of the kinetic equation. The issue of physical constraints such as Galilean invariance is addressed and an active-learning procedure is introduced to help ensure that the dataset used is representative enough. The reduced system takes the form of a conventional moment system and works regardless of the numerical discretization used. Numerical results are presented for the BGK (Bhatnagar–Gross–Krook) model and binary collision of Maxwell molecules. We demonstrate that the reduced model achieves a uniform accuracy in a wide range of Knudsen numbers spanning from the hydrodynamic limit to free molecular flow.",
    "issued": {
      "date-parts": [["2019"]]
    },
    "journalAbbreviation": "Proc. Natl. Acad. Sci.",
    "page": "21983-21991",
    "issue": "44",
    "volume": "116",
    "citekey": "Han.Proceedings of the National Academy of Sciences.2019",
    "id": "85348b78-36e5-4677-90f6-c30893603ee2",
    "tags": [
      "Boltzmann Equations",
      "Model Reduction",
      "Hydrodynamic Approximation",
      "Generalized Moments",
      "Deep Learning"
    ]
  },
  {
    "type": "paper-conference",
    "author": [
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Wang",
        "given": "Zhiwei"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      }
    ],
    "title": "An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural Network",
    "container-title": "Proceedings of Mathematical and Scientific Machine Learning",
    "abstract": "Deep neural network (DNN) usually learns the target function from low to high frequency, which is called frequency principle or spectral bias. This frequency principle sheds light on a high-frequency curse of DNNs -- difficult to learn high-frequency information. Inspired by the frequency principle, a series of works are devoted to develop algorithms for overcoming the high-frequency curse. A natural question arises: what is the upper limit of the decaying rate w.r.t. frequency when one trains a DNN? In this work, our theory, confirmed by numerical experiments, suggests that there is a critical decaying rate w.r.t. frequency in DNN training. Below the upper limit of the decaying rate, the DNN interpolates the training data by a function with a certain regularity. However, above the upper limit, the DNN interpolates the training data by a trivial function, i.e., a function is only non-zero at training data points. Our results indicate a better way to overcome the high-frequency curse is to design a proper pre-condition approach to shift high-frequency information to low-frequency one, which coincides with several previous developed algorithms for fast learning high-frequency information. More importantly, this work rigorously proves that the high-frequency curse is an intrinsic difficulty of DNNs.",
    "issued": {
      "date-parts": [["2022"]]
    },
    "page": "205--214",
    "volume": "190",
    "publisher": "PMLR",
    "event": "Proceedings of Machine Learning Research",
    "citekey": "pmlr-v190-luo22a",
    "id": "7452f12c-22ab-46c7-aa4c-2471c72ba6ae",
    "tags": ["Deep Learning Theory", "F-Principle"]
  },
  {
    "type": "article-journal",
    "title": "Explicitizing an Implicit Bias of the Frequency Principle in Two-layer Neural Networks",
    "author": [
      {
        "family": "Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.1905.10264",
    "arXiv": "1905.10264",
    "abstract": "It remains a puzzle that why deep neural networks (DNNs), with more parameters than samples, often generalize well. An attempt of understanding this puzzle is to discover implicit biases underlying the training process of DNNs, such as the Frequency Principle (F-Principle), i.e., DNNs often fit target functions from low to high frequencies. Inspired by the F-Principle, we propose an effective model of linear F-Principle (LFP) dynamics which accurately predicts the learning results of two-layer ReLU neural networks (NNs) of large widths. This LFP dynamics is rationalized by a linearized mean field residual dynamics of NNs. Importantly, the long-time limit solution of this LFP dynamics is equivalent to the solution of a constrained optimization problem explicitly minimizing an FP-norm, in which higher frequencies of feasible solutions are more heavily penalized. Using this optimization formulation, an a priori estimate of the generalization error bound is provided, revealing that a higher FP-norm of the target function increases the generalization error. Overall, by explicitizing the implicit bias of the F-Principle as an explicit penalty for two-layer NNs, our work makes a step towards a quantitative understanding of the learning and generalization of general DNNs.",
    "issued": {
      "date-parts": [["2019"]]
    },
    "journalAbbreviation": "arXiv",
    "citekey": "Zhang.arXiv.2019",
    "id": "8c877f5f-c94f-4718-98b4-e4bebdc4e70f",
    "tags": ["Deep Learning Theory", "Implicit Bias"]
  },
  {
    "type": "paper-conference",
    "author": [
      {
        "family": "Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "title": "A type of generalization error induced by initialization in deep neural networks",
    "container-title": "Proceedings of Mathematical and Scientific Machine Learning",
    "abstract": "How initialization and loss function affect the learning of a deep neural network (DNN), specifically its generalization error, is an important problem in practice. In this work, by exploiting the linearity of DNN training dynamics in the NTK regime \\citep{jacot2018neural,lee2019wide}, we provide an explicit and quantitative answer to this problem. Focusing on regression problem, we prove that, in the NTK regime, for any loss in a general class of functions, the DNN finds the same \\emph{global} minima---the one that is nearest to the initial value in the parameter space, or equivalently, the one that is closest to the initial DNN output in the corresponding reproducing kernel Hilbert space. Using these optimization problems, we quantify the impact of initial output and prove that a random non-zero one increases the generalization error. We further propose an antisymmetrical initialization (ASI) trick that eliminates this type of error and accelerates the training. To understand whether the above results hold in general, we also perform experiments for DNNs in the non-NTK regime, which demonstrate the effectiveness of our theoretical results and the ASI trick in a qualitative sense. Overall, our work serves as a baseline for the further investigation of the impact of initialization and loss function on the generalization of DNNs, which can potentially guide and improve the training of DNNs in practice.",
    "issued": {
      "date-parts": [["2020"]]
    },
    "page": "144--164",
    "volume": "107",
    "publisher": "PMLR",
    "event": "Proceedings of Machine Learning Research",
    "id": "ba05d7dc-69af-45b6-8303-b8becaadfd59",
    "tags": ["Deep Learning Theory", "ASI", "NTK"]
  },
  {
    "type": "article-journal",
    "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks",
    "author": [
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Xiao",
        "given": "Yanyang"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "Communications in Computational Physics",
    "ISSN": "1815-2406",
    "DOI": "10.4208/cicp.oa-2020-0085",
    "arXiv": "1901.06523",
    "abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) -- DNNs often fit target functions from low to high frequencies -- on high-dimensional benchmark datasets such as MNIST/CIFAR10 and deep neural networks such as VGG16. This F-Principle of DNNs is opposite to the behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibit faster convergence for higher frequencies for various scientific computing problems. With a simple theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.",
    "issued": {
      "date-parts": [["2020"]]
    },
    "journalAbbreviation": "Commun. Comput. Phys.",
    "page": "1746-1767",
    "issue": "5",
    "volume": "28",
    "citekey": "Xu.arXiv.2019",
    "id": "91a21d1d-882f-4d15-a653-20d314cc144f",
    "tags": ["Deep Learning Theory", "Fourier Analysis", "Frequency Principle"]
  },
  {
    "type": "article-journal",
    "title": "A fast spectral method for the inelastic Boltzmann collision operator and application to heated granular gases",
    "author": [
      {
        "family": "Hu",
        "given": "Jingwei"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "Journal of Computational Physics",
    "ISSN": "0021-9991",
    "DOI": "10.1016/j.jcp.2019.01.049",
    "abstract": "In this paper, we propose a simple fast Fourier spectral method for the inelastic Boltzmann collision operator, with its application to one of the widely used models of granular gases, the inelastic Boltzmann equation with a heating source. Compared to the direct Fourier spectral method, our fast algorithm reduces the computational complexity from O ( N 6 ) to O ( M N 4 log ⁡ N ) per evaluation of the collision operator in three dimensions, where N is the number of discretization points in each velocity dimension and M ≪ N 2 is the number of quadrature points used on the unit sphere. We test the numerical accuracy and efficiency of the proposed method in both two dimensional and three dimensional examples, where in the latter case the famous Haff's cooling law for granular flows is successfully recovered.",
    "issued": {
      "date-parts": [["2019"]]
    },
    "journalAbbreviation": "J. Comput. Phys.",
    "page": "119-134",
    "volume": "385",
    "citekey": "Hu.Journal of Computational Physics.2019",
    "id": "740c784d-ddc5-4949-9644-85d3dfeb391c",
    "tags": ["Boltzmann Equations", "Spectral Methods", "Fast Algorithms"]
  },
  {
    "type": "article-journal",
    "title": "An improved semi-Lagrangian time splitting spectral method for the semi-classical Schrödinger equation with vector potentials using NUFFT",
    "author": [
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Zhang",
        "given": "Yong"
      },
      {
        "family": "Zhou",
        "given": "Zhennan"
      }
    ],
    "container-title": "Applied Numerical Mathematics",
    "ISSN": "0168-9274",
    "DOI": "10.1016/j.apnum.2016.08.015",
    "abstract": "In this paper, we propose a new time splitting Fourier spectral method for the semi-classical Schrödinger equation with vector potentials. Compared with the results in [21], our method achieves spectral accuracy in space by interpolating the Fourier series via the NonUniform Fast Fourier Transform (NUFFT) algorithm in the convection step. The NUFFT algorithm helps maintain high spatial accuracy of Fourier method, and at the same time improve the efficiency from O(N2) (of direct computation) to O(Nlog⁡N) operations, where N is the total number of grid points. The kinetic step and potential step are solved by analytical solution with pseudo-spectral approximation, and, therefore, we obtain spectral accuracy in space for the whole method. We prove that the method is unconditionally stable, and we show improved error estimates for both the wave function and physical observables, which agree with the results in [3] for vanishing potential cases and are superior to those in [21]. Extensive one and two dimensional numerical studies are presented to verify the properties of the proposed method, and simulations of 3D problems are demonstrated to show its potential for future practical applications.",
    "issued": {
      "date-parts": [["2017"]]
    },
    "journalAbbreviation": "Appl. Numer. Math.",
    "page": "144-159",
    "volume": "111",
    "citekey": "Ma.Applied Numerical Mathematics.2017hfc",
    "id": "280cfd67-577b-4c0e-b52a-a9a8c44cbeb6",
    "tags": [
      "Semi-classical Schrödinger",
      "Time Splitting Spectral Methods",
      "NUFFT"
    ]
  },
  {
    "type": "article-journal",
    "title": "Uniform spectral convergence of the stochastic Galerkin method for the linear transport equations with random inputs in diffusive regime and a micro–macro decomposition-based asymptotic-preserving method",
    "author": [
      {
        "family": "Jin",
        "given": "Shi"
      },
      {
        "family": "Liu",
        "given": "Jian-Guo"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "Research in the Mathematical Sciences",
    "DOI": "10.1186/s40687-017-0105-1",
    "abstract": "In this paper we study the stochastic Galerkin approximation for the linear transport equation with random inputs and diffusive scaling. We first establish uniform (in the Knudsen number) stability results in the random space for the transport equation with uncertain scattering coefficients and then prove the uniform spectral convergence (and consequently the sharp stochastic asymptotic-preserving property) of the stochastic Galerkin method. A micro–macro decomposition-based fully discrete scheme is adopted for the problem and proved to have a uniform stability. Numerical experiments are conducted to demonstrate the stability and asymptotic properties of the method.",
    "issued": {
      "date-parts": [["2017"]]
    },
    "journalAbbreviation": "Res. Math. Sci.",
    "page": "15",
    "issue": "1",
    "volume": "4",
    "citekey": "Jin.Research in the Mathematical Sciences.2017",
    "id": "5ef9d0de-baa7-4017-a675-de5caf0046fc",
    "tags": ["gPC-SG", "APUQ", "Numerical Analysis"]
  },
  {
    "type": "article-journal",
    "title": "Explicit and Implicit TVD Schemes for Conservation Laws with Caputo Derivatives",
    "author": [
      {
        "family": "Liu",
        "given": "Jian-Guo"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Zhou",
        "given": "Zhennan"
      }
    ],
    "container-title": "Journal of Scientific Computing",
    "ISSN": "0885-7474",
    "DOI": "10.1007/s10915-017-0356-4",
    "abstract": "In this paper, we investigate numerical approximations of the scalar conservation law with the Caputo derivative, which introduces the memory effect. We construct the first order and the second order explicit upwind schemes for such equations, which are shown to be conditionally ℓ1 contracting and TVD. However, the Caputo derivative leads to the modified CFL-type stability condition, (Δt)α=O(Δx), where α∈(0,1] is the fractional exponent in the derivative. When α is small, such strong constraint makes the numerical implementation extremely impractical. We have then proposed the implicit upwind scheme to overcome this issue, which is proved to be unconditionally ℓ1 contracting and TVD. Various numerical tests are presented to validate the properties of the methods and provide more numerical evidence in interpreting the memory effect in conservation laws.",
    "issued": {
      "date-parts": [["2017"]]
    },
    "journalAbbreviation": "J. Sci. Comput.",
    "page": "291-313",
    "issue": "1",
    "volume": "72",
    "citekey": "Liu.Journal of Scientific Computing.2017",
    "id": "8424618b-9d0c-4d78-bb18-dabbbfde601a",
    "tags": ["Conservation Laws", "TVD", "Fractional"]
  },
  {
    "type": "article-journal",
    "title": "Phase Diagram for Two-layer ReLU Neural Networks at Infinite-width Limit",
    "author": [
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      }
    ],
    "container-title": "Journal of Machine Learning Research",
    "issued": {
      "date-parts": [["2021", "2", "1"]]
    },
    "page": "1-47",
    "volume": "22",
    "id": "a7d2330e-89bb-44a5-9db8-641647467dac",
    "tags": ["Deep Learning Theory", "Phase Diagram"]
  },
  {
    "type": "article-journal",
    "title": "A Linear Frequency Principle Model to Understand the Absence of Overfitting in Neural Networks",
    "author": [
      {
        "family": "Zhang",
        "given": "Yaoyu"
      },
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      }
    ],
    "container-title": "Chinese Physics Letters",
    "ISSN": "0256-307X",
    "DOI": "10.1088/0256-307x/38/3/038701",
    "arXiv": "2102.00200",
    "abstract": "Why heavily parameterized neural networks (NNs) do not overfit the data is an important long standing open question. We propose a phenomenological model of the NN training to explain this non-overfitting puzzle. Our linear frequency principle (LFP) model accounts for a key dynamical feature of NNs: they learn low frequencies first, irrespective of microscopic details. Theory based on our LFP model shows that low frequency dominance of target functions is the key condition for the non-overfitting of NNs and is verified by experiments. Furthermore, through an ideal two-layer NN, we unravel how detailed microscopic NN training dynamics statistically gives rise to an LFP model with quantitative prediction power.",
    "issued": {
      "date-parts": [["2021"]]
    },
    "journalAbbreviation": "Chin. Phys. Lett.",
    "page": "038701",
    "issue": "3",
    "volume": "38",
    "citekey": "Zhang.Chinese Physics Letters.2021",
    "id": "fac935e7-d24b-4f1a-ad27-c0152ef9bda5",
    "tags": ["Deep Learning Theory", "Frequency Principle"]
  },
  {
    "type": "article-journal",
    "title": "Theory of the Frequency Principle for General Deep Neural Networks",
    "author": [
      {
        "family": "Luo",
        "given": "Tao"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Xu",
        "given": "Zhi-Qin John"
      },
      {
        "family": "Zhang",
        "given": "Yaoyu"
      }
    ],
    "container-title": "CSIAM Transactions on Applied Mathematics",
    "ISSN": "2708-0560",
    "DOI": "10.4208/csiam-am.so-2020-0005",
    "issued": {
      "date-parts": [["2021"]]
    },
    "journalAbbreviation": "CSIAM Trans. Appl. Math.",
    "page": "484-507",
    "issue": "3",
    "volume": "2",
    "citekey": "Luo.CSIAM Transactions on Applied Mathematics.2021",
    "id": "0959bff4-6f23-44c8-ab97-9825b3149c37",
    "tags": ["Deep Learning Theory", "F-principle"]
  },
  {
    "type": "chapter",
    "title": "Trails in Kinetic Theory, Foundational Aspects and Numerical Methods",
    "author": [
      {
        "family": "Carrillo",
        "given": "José Antonio"
      },
      {
        "family": "Hu",
        "given": "Jingwei"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      },
      {
        "family": "Rey",
        "given": "Thomas"
      }
    ],
    "ISBN": "9783030671037",
    "abstract": "Over the past decades, kinetic description of granular materials has received a lot of attention in mathematical community and applied fields such as physics and engineering. This article aims to review recent mathematical results in kinetic granular materials, especially for those which arose since the last review Villani (J Stat Phys 124(2):781–822, 2006) by Villani on the same subject. We will discuss both theoretical and numerical developments. We will finally showcase some important open problems and conjectures by means of numerical experiments based on spectral methods.",
    "issued": {
      "date-parts": [["2021"]]
    },
    "page": "1-36",
    "collection-title": "SEMA SIMAI Springer Series",
    "DOI": "10.1007/978-3-030-67104-4_1",
    "citekey": "Carrillo.SEMA SIMAI Springer Series.2021",
    "id": "9b395c92-961d-419d-a010-42ae52007676",
    "tags": ["Kinetic Theory", "Granular Materials", "Spectral Methods"]
  },
  {
    "type": "article-journal",
    "title": "The Discrete Stochastic Galerkin Method for Hyperbolic Equations with Non-smooth and Random Coefficients",
    "author": [
      {
        "family": "Jin",
        "given": "Shi"
      },
      {
        "family": "Ma",
        "given": "Zheng"
      }
    ],
    "container-title": "Journal of Scientific Computing",
    "ISSN": "0885-7474",
    "DOI": "10.1007/s10915-017-0426-7",
    "abstract": "We develop a general polynomial chaos (gPC) based stochastic Galerkin (SG) for hyperbolic equations with random and singular coefficients. Due to the singular nature of the solution, the standard gPC-SG methods may suffer from a poor or even non convergence. Taking advantage of the fact that the discrete solution, by the central type finite difference or finite volume approximations in space and time for example, is smoother, we first discretize the equation by a smooth finite difference or finite volume scheme, and then use the gPC-SG approximation to the discrete system. The jump condition at the interface is treated using the immersed upwind methods introduced in Jin (Proc Symp Appl Math 67(1):93–104, 2009) and Jin and Wen (Commun Math Sci 3:285–315, 2005). This yields a method that converges with the spectral accuracy for finite mesh size and time step. We use a linear hyperbolic equation with discontinuous and random coefficient, and the Liouville equation with discontinuous and random potential, to illustrate our idea, with both one and second order spatial discretizations. Spectral convergence is established for the first equation, and numerical examples for both equations show the desired accuracy of the method.",
    "issued": {
      "date-parts": [["2018"]]
    },
    "journalAbbreviation": "J. Sci. Comput.",
    "page": "97-121",
    "issue": "1",
    "volume": "74",
    "citekey": "Jin.Journal of Scientific Computing.2018",
    "id": "f7d99ade-cc82-4ba0-a3d0-f5ff63584516",
    "tags": ["Hyperbolic Equations", "Uncertainty Quantification", "gPC-SG"]
  }
]
